import numpy as np
from sklearn.model_selection import train_test_split

Y=(Y/10).round()

X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.1)

Y_train=torch.from_numpy(Y_train.astype(np.float32))
Y_test=torch.from_numpy(Y_test.astype(np.float32))

class LogisticRegression(nn.Module):

    def __init__(self,n_input_features):
        super(LogisticRegression, self).__init__()
        self.linear=nn.Linear(n_input_features,1)
   
    def forward(self,x):
        y_predicted=torch.sigmoid(self.linear(x))
        return y_predicted
    
model=LogisticRegression(X.shape[1])

learning_rate=0.01
criterion=nn.BCELoss()
optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)


# training loop
num_epochs=10
for epoch in range(num_epochs):
    
    loss=0
    
    for i in range(int(np.ceil(X_train.shape[0]/100))):
        
        # forward pass and loss
        Y_predicted=model(torch.from_numpy(X[range((i-1)*100,i*100)].toarray().astype(np.float32)))
        loss+=criterion(Y_predicted, np.transpose(Y_train[range((i-1)*100,i*100)]))
    
    # backward pass
    loss.backward()
    
    # updates
    optimizer.step()
    
    #zero gradients
    optimizer.zero_grad()
    

    print(f'epoch: {epoch+1}, loss= {loss.item():.4f}')
    
    with torch.no_grad():
        Y_predicted_cls=[]
        for i in range(int(np.ceil(X_test.shape[0]/100))):
            Y_predicted = model(torch.from_numpy(X_test[range((i-1)*100,i*100)].toarray().astype(np.float32)))
            Y_predicted_cls.append((10*Y_predicted).round()/10)
        Y_predicted_cls=torch.reshape(torch.cat(Y_predicted_cls,dim=0),(-1,))
        acc = Y_predicted_cls.eq(Y_test).sum()/float(Y_test.shape[0])
        print(f'accuracy={acc:.4f}')
